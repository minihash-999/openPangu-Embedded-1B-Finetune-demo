# CCLoRA

## 问题分析

LoRA微调算法通过更新附加在冻结预训练模型权重上的低秩矩阵，实现对模型的高效微调。过程中串行执行冻结预训练权重及低秩权重的前向和反向，在分布式场景，会造成冗余通信耗时。实际预训练分支与更新分支没有依赖关系，可通过异步方式实现通算优化；此外，通过挖掘数学等价的计算方式，可覆盖各种场景，实现极致性能优化。

## 解决方案

1. 将单一流水优化为通信计算双流水线
![输入图片说明](https://foruda.gitee.com/images/1722944695028060121/d24c8bcf_8362322.png "CCLoRA.png")

2. 数学等价方式，合并通信，从而利用MC2进一步加速

   SP&Row场景，B是线性层，各卡参数一致，通过变换后仅需进行一次allgather(dy)
3. 数学等价方式，优化scale逻辑
![输入图片说明](https://foruda.gitee.com/images/1725264240434981872/0b3fda9a_8362322.png "公式.png")

## 使用方法

RC2以上版本，LoRA微调场景，算法与PP、VPP、分布式优化器等场景兼容

通过设置--lora-fusion开启CCLoRA的加速

## 使用效果

|   模型    | NPU  |  TP  |  PP  | VPP | 分布式优化器 | 基线吞吐 | CCLoRA吞吐 | 性能提升 |
| :-------: | :--: | :--: | :--: | :--: | :----------: | :------: | :--------: | :------: |
| Llama2-7B |  8   |  8   |  1   |   |              |   5.42   |    5.83    |   7.6%   |
| Qwen-32B  |  8   |  8   |  1   |   |              |   1.47   |    1.61    |   9.7%   |
| Llama2-7B |  4   |  2   |  2   |   |              |   2.73   |    2.94    |   7.7%   |
| Llama2-7B |  4   |  2   |  1   |   |      √       |   2.82   |    3.04    |   7.8%   |
| Llama2-7B |  8   |  2   |  4   | 2 |              |   5.15   |    5.51    |   7.0%   |
| Llama2-70B动态序列 |  8   |  2   |  4   | / |              |   6.42  |    7.51    |   17.0%   |

